{"cells":[{"cell_type":"markdown","source":"## Introduction","metadata":{"deepnote_cell_type":"markdown","cell_id":"00000-6ac93d33-7180-4000-8c7f-2b27aef6eacd"}},{"cell_type":"markdown","source":"Every science and engineering discipline relies on differentiation in some capacity, whether seeking to optimize system operations, deriving rates of change, or evaluating complex expressions. In this era of abundant computationally intensive tasks, evaluating gradients of any function (regardless of form) is both practical and valuable. The FADiff package addresses this task by automatically differentiating functions using forward mode. By implementing automatic differentiation (AD), which sequentially evaluates elementary functions, FADiff avoids the complexity of symbolic differentiation and the precision issues of numerical differentiation. Additional information on implementation is below.","metadata":{"deepnote_cell_type":"markdown","cell_id":"00001-ed0a2f06-a5ae-4e02-8b83-0f52f76a92f1"}},{"cell_type":"markdown","source":"## Background","metadata":{"deepnote_cell_type":"markdown","cell_id":"00002-23402e9e-6c54-4fd5-8a52-e3294fe77c87"}},{"cell_type":"markdown","source":"Automatic Differentiation (AD) is a set of techniques for evaluating derivatives precisely based on computation graphs, chain rules and other symbolic rules. Compared with manual calculation or symbolic approach to calculating derivatives, it is highly convenient and fast since it frees users from tedius calculation and proof. Compared with finite approximation (a.k.a numerical differentiation), it is more accurate in that it avoids truncation errors or rounding-off errors that might arouse in symbolic differentiation when selecting a huge step (h) or a tiny step (h). (We've analyzed this point in HW4.). Due to these advantages, it has been widely used in scientific computing, machine learning, deep learning, etc. \n\nThe mathematical background knowledge mainly includes matrix-vector product, Jacobian matrix, the algegra of dual numbers, Taylor's series expansion, higher-order derivatives, etc. We will discuss them in more details later. There are 2 evaluation modes in AD, **forward mode** and **reverse mode**.\n\n1. Forward mode performs the operation of evaluating the numerical derivative concurrently with evaluating the function itself on a computational graph.\n\n2. Reverse mode is an alternative to the forward mode. It uses the computation graph in forward mode to calculate the final output and then traveres reversely to perform to operation of evaluating derivatives. This mode is commonly used in deep learning and neural networks, in which it is also called backpropogation. \n\n\n","metadata":{"deepnote_cell_type":"markdown","cell_id":"00003-6a571975-82fb-4224-a696-3d6ed4f631a8"}},{"cell_type":"markdown","source":"### 1. Matrix-vector Products\n##### 1.1 Definition \n   Given an $m\\times n$  matrix $A_{m\\times n}$ and a vector $x\\in R^{n}$, there is a way to create a linear combination\n   $$\n   x_1a_1 + x_2a_2 + ... + x_na_n \\in R^m \n   $$\n   using the columns $a_1, . . . , a_n$ of $A$, where $x=\\left[x_1,x_2,...,x_n \\right]^{T}$.\n\n##### 1.2 Notes\n1. Matrix-vector products are only valid when the sizes of the matrix and vector are compatible – the number of elements of vector $x$ must equal the number of columns of matrix $A$. The number of elements in the output vector must equal to the number of rows in matrix $A$.\n2. We can interpret the matrix-vector products as creating a linear transformation or a **map** from $R^n$ to $R^m$","metadata":{"deepnote_cell_type":"markdown","cell_id":"00004-5db04178-d588-410c-8783-67c6ad616879"}},{"cell_type":"markdown","source":"### 2. Two Evaluation Mode: Forward & Reverse, Jacobian Matrix.\nAutomatic Differentiation (AD) can be applied on both scalar functions with one variable or functions with multiple variables. The derivative calculation of a single variable is super straight forward, while in the situations with multiple\nvariables, we will introduce a terminology called Jacobian Matrix ($J$).\n\n\nLet's start from a general case with $x$ is a vector)","metadata":{"deepnote_cell_type":"markdown","cell_id":"00005-c19b5860-9b26-4150-b5fc-b37d5a98eaae"}},{"cell_type":"markdown","source":"##### 2.1 Jacobian Matrix\nIf f is a matrix of multiple functions with multiple input variables, then denote $f$ as \n$$\nf=\\begin{bmatrix} f_1(x,y) \\\\ f_2(x,y) \\end{bmatrix}\n$$\nThen, the derivative of matrix f is called Jacobian Matrix $J$:\n$$\n\\begin{aligned}\n  J = \n  \\begin{bmatrix}\n    \\partial f_{1} / \\partial x & \\partial f_{1} / \\partial y \\\\\n    \\partial f_{2} / \\partial x & \\partial f_{2} / \\partial y\n  \\end{bmatrix}\n\\end{aligned}\n$$","metadata":{"deepnote_cell_type":"markdown","cell_id":"00006-0d13ad32-d233-4e83-b04c-b702681e735b"}},{"cell_type":"markdown","source":"##### 2.2 Forward Mode\nA program can be written as a combination of several functions: $f = f_1 ... f_n$, let's set $x_0$ is a vector in $R^n$, $x_n$ is the output vector, each $f_i$ is the transaction function (a generalized \"matrix\" from the definition of matrix-vector products), then \n$$\nx_1 = f_1x_0\n$$\n$$\nx_2 = f_2x_1\n$$\n$$\n...\n$$\n$$\nx_n=f_nx_{n-1}.\n$$\nFrom the chain rule, we have:\n$$\n\\dot{x_1} =  (J f_1 x_0)\n$$\n$$\n\\dot{x_2} =  (J f_2 x_1) \\times \\dot{x_1}\n$$\n$$\n ... \n $$\n$$ \n\\dot{x_n} = (J f_n x_{n-1})\\times \\dot{x_{n-1}}. \n$$\n\nThe above process of evaluating derivatives is called **forward mode Automatic Differentiation**.\n\n\n","metadata":{"deepnote_cell_type":"markdown","cell_id":"00007-9d2f9f92-aa61-4684-b53e-c75e364f3a76"}},{"cell_type":"markdown","source":"##### 2.3 Reverse Mode\nIf we take transpose on both left and right sides of equation (1),(2)...(n) above, then \n$${x_1}^\\prime = (f_1x_0)^T$$\n$${x_2}^\\prime = (f_2x_1)^T$$\n$$...$$\n$${x_n}^\\prime = (f_nx_{n-1})^T.$$\nFrom the chain rule, we have:\n$$ {x_{n-1}}^\\prime =  (J f_n x_{n-1})^T$$\n$$ {x_{n-2}}^\\prime =  (J f_{n-1} x_{n-2}) \\times {x_{n-1}}^\\prime $$\n$$ ... $$\n$$ {x_0}^\\prime = (J f_1 x_0)\\times {x_1}^\\prime. $$\n\nThe above process of evaluating derivatives is called **reverse mode Automatic Differentiation**.\n\n\n","metadata":{"deepnote_cell_type":"markdown","cell_id":"00008-00086732-90a9-495c-a984-3f19577fcfa7"}},{"cell_type":"markdown","source":"##### 2.4 Example of computational graph, forward and reverse mode.\n\n![image.png](attachment:image.png)\n","metadata":{"deepnote_cell_type":"markdown","cell_id":"00009-fdd51c6b-78f1-474f-ae62-67a3d506f38d"}},{"cell_type":"markdown","source":"##### 2.5 When to use reverse or forward mode?\n\nThe difference between forward and reverse mode lies in the start point of matrix multiplication. \nFrom the view of the times of multiplication operation, when the dimension of input is less than that of the output, forward mode has less multiplication operations than reverse mode; comparably, when the dimension of input is more than that of the output, reverse mode has less multiplication operations. \nTherefore, when the dimension of input is less than that of the output, forward mode is more efficient; when the dimension of input is more than that of the output, reverse mode is more efficient. ","metadata":{"deepnote_cell_type":"markdown","cell_id":"00010-1ab825af-5c90-4cc1-89de-d18db6fc648e"}},{"cell_type":"markdown","source":"### 3. The algebra of dual number \n##### 3.1 Definition\nA dual number (z) is composed of a real part (a) and a dual part (b).  We denote it as $$z = a + \\epsilon b$$.\n\n##### 3.3 What's the effect of dual numbers on derivatives? \nThe usage of dual number augments the arithmetic in real number space to any input and allows the user to get the derivatives without calculating them. A function f(x) where x is a dual number can be re-written in a dual number format, where the real component is the function and dual component contains the derivative (as we discussed in lecture 10).\n\nGenerally, let $\\hat f$ denotes the expansion of real-value function $f$ to dual number space, then\n$$ \\hat f(x_1+x_1^\\prime\\epsilon, ..., x_n + x_n^\\prime\\epsilon):=f(x_1,...,x_n)+\\dot f(x_1,...x_n) \\left(\\begin{array}{c}\n    x_1^\\prime\\\\ \n    .\\\\\n    .\\\\\n    .\\\\\n    x_n^\\prime\\\\\n  \\end{array}\\right)\\epsilon $$\n\nIf $f$ is a matrix of multiple differentiable functions, then we can extend the above framework by replacing $\\dot f(x_1,...x_n)$ with $J f (x_1,...,x_n)$\n","metadata":{"deepnote_cell_type":"markdown","cell_id":"00011-359781dc-757d-4501-98ce-46f0bf896ad7"}},{"cell_type":"markdown","source":"### 4. Elemental functions\n\nAutomatic Differentiation relies on the fact that we've already know the derivative at each step. So, we need some elemental functions. A function is called elemental function if it always returns the same result for same argument values and it has no side effect like modifying a global variable, etc. The result of calling a elemental function is the exact return value. \nSome examples are pow(), mean(), sqrt(), while printf(), rand() and time() are not elemental functions.\n","metadata":{"deepnote_cell_type":"markdown","cell_id":"00012-e34784ab-8ccf-42bb-9b39-60d417ca1521"}},{"cell_type":"markdown","source":"## How to use FADIFF\n","metadata":{"deepnote_cell_type":"markdown","cell_id":"00013-7e1db5fe-fdad-4060-a893-2c835adafd7a"}},{"cell_type":"markdown","source":"We expect the use of our package, `FADiff`,\nto be largely through its API. Where necessary or practical,\nour API may permit the use of objects and functions from NumPy or other\nwidely-used external libraries. However, for certain areas of our\nimplementation, we expect our package to require the exclusive use of\ninternally defined objects and functions. For example, we might prohibit users\nfrom using external libraries for elementary functions (e.g., sine and cos)\nwith variables and\nonly allow them to use our package’s implementations for such functions. This\nmay help to reduce the potential for issues further on in the development\nprocess such as disuse or misuse of our package’s operator-overloaded\nfunctions, among other things. We will be clear in our documentation on how\nthe user should use our package’s API including the proper use of variables\nand methods.\n","metadata":{"deepnote_cell_type":"markdown","cell_id":"00014-fd350bd6-9b0a-4418-a961-451ba7f873f1"}},{"cell_type":"markdown","source":"### Run\nOur package can be downloaded from\n[https://github.com/teamxvii/cs107-FinalProject]\n(https://github.com/teamxvii/cs107-FinalProject) (make sure you are signed in\nto your GitHub account where you are a collaborator for our project). There you will see a green\nbutton called 'Code' and a button further to the left of that which should say\n'master' (if not click on it to switch it to the 'master' branch). Click on the\n'Code' button and then click on 'Download ZIP' in the window that pops up. This\nwill download the package to your computer. Unzip its contents to\nthe location of your code that will be using the package. For importability,\nrename the unzipped folder to something that does not contain hyphens. From\nhere on out, it will be referred to as `cs107_FinalProject`. In your code file,\nonly one header should be needed to import and use our entire package such as\n`from cs107_FinalProject.code.FADiff import *`.\nIn addition, we implemented our package with\nPython 3.8.2 on Linux but other versions of Python may still be compatible.\nTo retrieve the dependencies used in our package, navigate to the\n`cs107_FinalProject` folder in a terminal window and run the following:\n\nFor Python 2 --\n```\npip install -r requirements.txt\n```\nFor Python 3 --\n```\npip3 install -r requirements.txt\n```\n\nAn example\ndemonstrating the use of our package is shown below (the following code would\nbe in your code file):\n\n```\nfrom cs107_FinalProject.code.FADiff import *      # Imports our package\n\nx = FADiff(5, 1)      # Creates a variable x with value of 5 and derivative of 1\nf = x + 2             # Creates a function f using variable x above\nprint(f.val)          # Prints value of f\nprint(f.der)          # Prints derivative of f\n```\n\nTo run the above code, in a terminal window, navigate to the folder that contains\nthe `cs107_FinalProject` folder and your code file and run the following:\n\nFor Python 2 --\n```\npython your_code_file.py\n```\nFor Python 3 --\n```\npython3 your_code_file.py\n```\n\nwhere 'your_code_file' is the name of your code file. The following output should\nthen be rendered:\n\n```\n7\n1\n```\n","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00015-b2245acc-d1d9-4f25-bace-5aef24e94592"}},{"cell_type":"markdown","source":"### Test\nNavigate to the `cs107_FinalProject/code` folder in a terminal window and run the\nfollowing:\n\n```\npytest test_main.py\n```\n","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00016-189a523d-6586-47c7-825b-bec625389f54"}},{"cell_type":"markdown","source":"## Software Organization\n\n### 1. What will the directory structure look like?\nCurrently, our directory structure looks like the following:\n```\ncs107_FinalProject/\n    code/\n        FADiff.py\n        test_main.py\n    docs/\n        milestone1.ipynb\n        milestone2.ipynb\n        milestone2_progress.ipynb\n    requirements.txt\n    README.md\n```\nHowever, we anticipate the directory structure to eventually look something like:\n```\ncs107_FinalProject/\n    src/\n        FADiff/\n            FADiff.py\n    includes/\n    docs/\n        milestone1.ipynb\n        milestone2.ipynb\n        milestone2_progress.ipynb\n    tests/\n        test_main.py\n    examples/\n    requirements.txt\n    README.md\n    LISCENSE.md\n    .travis.yml\n    setup.py\n```\n### 2. What modules do you plan on including? What is their basic functionality?\nOur FADiff package contains a module named `FADiff.py`. `FADiff.py` will\ncontain our main automatic differentiation class `FADiff()` as well as\nelementary functions that are used to calculate the derivatives of\nall the elementary functions our package supports such as sine and cosine. Our package\nalso contains a module named `test_main.py` which contains our test class used in our\ntesting.\nWe also used NumPy as an external dependency for our calculations. As explained in the\n“How to Use FADiff” section earlier in this document, our implementation may need to\nlimit the use of external packages or only use them internally\n(i.e., hidden from the API) in certain areas moving forward.\n\n### 3. Where will your test suite live? Will you use TravisCI? CodeCov?\nCurrently, our tests are in the `cs107_FinalProject/code` folder in `test_main.py`.\nAs mentioned earlier, we eventually plan to have it live in the `/tests` directory of\nthe project and we used pytest for testing. Please see the\nHow to Use FADiff section earlier in this document for running tests.\n### 4. How will you distribute your package (e.g. PyPI)?\nWe are aiming to distribute through PyPI if time permits. \n### 5. How will you package your software? Will you use a framework? If so, which one and why? If not, why not?\nWe won't be packaging the software using any sort of framework. The code will be clonable and installable via GitHub and via PyPI if time permits.\n### 6. Other considerations?\nAfter finishing Homework 4 and some upcoming lectures on various software topics such as containers, we may consider revising our software organization later on.","metadata":{"deepnote_cell_type":"markdown","cell_id":"00015-f1214468-40c5-463f-a994-887828249efe"}},{"cell_type":"markdown","source":"## Implementation\n### 1. What are the core data structures?\nOur code currently does not take advantage of any particular data structure, but we will certainly improve our implementation in the next round of updates to do so. We plan on using a combination of tuples, numpy arrays, linked lists and/or dictionaries to build a directed graph. The graph will track the propagation of each variable at every intermediate step in a calculation. To build the graph, we plan on using a dictionary where each key contains the name of a node (intermediate step) and each value is a tuple with three elements: the value, the partial derivative with respect to the parent variable, and the key to the parent variable’s node. The value and partial derivative may be contained in a NumPy array if needed.\n\n### 2. What classes will you implement?\nWe have implemented an automatic differentiation class which is instantiated with two attributes, a value and a derivative. We may need to add a dictionary element to maintain the graph described above. Currently, the user is required to specify the seed vector, but we are discussing variations on this design choice.\n\n### 3. What method and name attributes will your classes have?\nClass methods include dunder methods for addition, subtraction, multiplication, division, power, negation and the corresponding right-hand versions of these, as well as specificed functions for sine, cosine, tangent, and exponentiation. In the next round of updates, we will add methods for sqrt, log, __str__ and __repr__.\n\n### 4. What external dependencies will you rely on?\nWe currently rely on NumPy for trig functions and exponentiation, and we will eventually use its array and linear algebra functions as well. We are considering also using Sphinx for auto-rendering and organizing our documentation.\n\n### 5. How will you deal with elementary functions like sin, sqrt, log, and exp (and all the others)?\nAs stated above, our package will rely on NumPy within class methods for these operations.\n","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00016-6429e878-746a-43e7-b72e-9564285c6a7c"}},{"cell_type":"markdown","source":"## Future Features\n\n### Things to impelement next\n\nIn this Milestone, we treat Jacobian as a scalar and it can only handle the case of single function of single input. Moving forward, we want to generalize the package for broader use cases. \n\n1. We will make the forward mode automatic differentiation object be able to access Jacobian Matrix.\n2. We will make the object to be for calculating partial direvatives. \n    1. The challenging part is how to handle the number of variables we have, e.g if we define a class to calculate derivatives for multivariable functions f(x,y,z) and f(x,y,z,m,n), \nwhich data structure should we use as the attribute of the class? \n    2. If we use an array instead of a scalar as the attribute, how can we implement the differentiation in an array?\n3. We might also think about how to calculate differentiation for polynomial functions like f(x) = x + sin(x) + cos(x).\n    1. The challenging part here is how to implement a dunder method to handle the order of add or substraction\n    2. Presumably, we might need to change the classes or change the data structures or add new modules. We should consult TAs and Prof. Sondak for more instruction and insights on implementation.\n\n\n","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00016-992c141d-2dc9-4765-8dd6-3e4610e49a06"}},{"cell_type":"markdown","source":"## Feedback\n\n### Milestone 1\n\n#### Comment\n\nGood job! Consider using a different data structure apart from list for your main data structure. You may want to use something a bit more robust and structured i.e. dictionary, tree, hashmap. Hopefully the upcoming homework will give you all a bit of insight into the functionalities/pros and cons of a data structure like a BST\n\nOluwatosin Alliyu , Oct 27 at 9:27pm\n\n#### Group Response\n\nPer Oluwatosin’s feedback, we are considering a dictionary or a tree-like structure as our main data structure for our evaluation trace. Due to the binary nature of primitive operations (i.e., two inputs produce an output), we hope a tree could possibly be used in that respect. Each node in a trace will have an elementary function value and its derivative. If we use a dictionary it will allow us to add a key-value pair where the key is the name of the node and the value is a tuple in which the first element will be the elementary function’s value and the second, its derivative. We would greatly appreciate advice or direction as to whether a tree or a dictionary would best satisfy our use case.\n","metadata":{"deepnote_cell_type":"markdown","cell_id":"00016-3f370fca-2823-4dfd-8fdd-b62b70ed4dcf"}}],"nbformat":4,"nbformat_minor":4,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.2"},"deepnote_notebook_id":"53cfc77d-cb46-4fc0-893b-180cbb28d26a","deepnote_execution_queue":[]}}